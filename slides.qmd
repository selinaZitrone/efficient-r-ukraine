---
title: "Efficient R"
subtitle: "Workshop for Ukraine"
author: "Selina Baldauf"
format: 
  revealjs:
    footer: "Selina Baldauf // Efficient R"
    mainfont: Cabinet Grotesk
    slide-number: true
    show-slide-number: all
    incremental: true
    self-contained: true
    code-line-numbers: false
    theme: slides.scss
    auto-stretch: false
    scrollable: true
execute: 
  echo: true
  eval: true
  cache: true
  message: false
knitr: 
  opts_chunk: 
    collapse: true
    comment: "#>" 
from: markdown+emoji
editor: source
---

```{r}
#| label: setup
#| include: false
#| cache: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))

library(tictoc)
```

## Welcome :construction:

::: {.columns}

::: {.column}

- Background in Ecology/Theoretical Ecology
- Scientific programmer at the Freie Universit√§t Berlin
- Research + Coding + Teaching
- Reach out via email or Bluesky
:::
::: {.column}

![](img/selina_cropped.jpg){width=50% fig-align="center"}

:::
:::

## What is efficiency?

$$
\textsf{efficiency} = \frac{\textsf{work done}}{\textsf{unit of effort}}
$$
<br>

:::{.columns}

:::{.column width="50%"}

:::{.fragment}

**Computational efficiency**

:::

:::{.fragment}

:computer: Computation time <br>
:floppy_disk: Memory usage

:::

:::

:::{.column width="50%"}

::: {.fragment}

**Programmer efficiency** 

üßç How long does it take to

:::

- *write* code?
- *maintain* code?
- *read* and *understand* the code?

:::


:::

. . .

**Tradeoffs** and **Synergies** between these types of efficiencies

---

:::{.r-stack}

![](img/pyramid_1.png){fig-align="center" .fragment}

![](img/pyramid_2.png){fig-align="center" .fragment}

![](img/pyramid_text.png){fig-align="center" .fragment}


:::

. . .

**Principles** and **tools** to make R programming more efficient for the :computer:

## Today :construction:

- How to **measure speed** of your code
- **Basics** of efficient R programming
- Efficient **data analysis**
- Advanced optimization
  - **Parallelization**
  - Integrating **C++**

. . .

:package: Material (Slides and Code) on Github<br>
:question: Question - just ask in the Chat or unmute yourself!

## Is R slow?

- R is slow compared to other programming languages (e.g. C++, Julia).

- R is the not the most memory efficient language

- R is designed to make statistical programming & data analysis **easy** and **interactive**, not fast
  
- But: [R is fast and memory efficient enough]{.highlight-purple} for most tasks.

## Should I optimize?

. . .

> It‚Äôs easy to get caught up in trying to remove all bottlenecks. Don‚Äôt! **Your time is valuable** and is **better spent analysing your data**, not eliminating possible inefficiencies in your code. **Be pragmatic**: don‚Äôt spend hours of your time to save seconds of computer time.<br>
(Hadley Wickham in [Advanced R](https://adv-r.hadley.nz/perf-improve.html))

. . .
<br>

#### Think about

- How much time do I **save** vs. **spend** optimizing? 
- **How often** do I run the code?
- Trade-offs between **readability** and **efficiency**

## Should I optimize?

If your code is too slow for you, you can go through these steps:

1. Can you just **run the code somewhere else**?

## Run the code somewhere else

- Run it on a cluster
- Run your script in the background

:::{.fragment}

Use RStudio **background jobs**

![](img/background_job.png){width=40%}

:::

:::{.fragment}

Start your R script from the command line

```bash
Rscript my_script.R
```
:::

## Should I optimize?

If your code is to slow for you, you can go through these steps:

:::{.nonincremental}

1. Can you just **run the code somewhere else**?

:::

2. **Identify bottlenecks** (i.e. slow parts) of your code
3. **Optimize** only those bottlenecks

# Profiling and benchmarking {.inverse}

> Measure the speed and memory usage of your code

## Profiling R code

What are the speed & memory **bottlenecks** in my code?

. . .

Use the [`profvis` package](https://rstudio.github.io/profvis/)

## Profiling R code

You can profile a section of code like this:

```{r eval=FALSE}
#| label: profvis
#| eval: false
#| code-line-numbers: "1-2|5-16|4,17"
# install.packages("profvis")
library(profvis)

profvis({
  # Create a data frame with 150 columns and 200000 rows
  df <- as.data.frame(matrix(rnorm(150 * 200000), nrow = 200000))

  # Calculate mean of each column
  means <- apply(df, 2, mean)

  # Subtract means
  for (i in seq_along(means)) {
    df[, i] <- df[, i] - means[i]
    # Simulate inefficiency by sleeping for 0.01 seconds
    Sys.sleep(0.01)
  }
})
```

## Profiling R code

Profvis **flame graph** shows time and memory spent in each line of code.

![](img/profvis-flame.png){width=80% fig-align="center"}

## Profiling R code

Profvis **data view** for details of the call stack.

![](img/profvis-data.png)

## Profiling R code

You can also interactively profile code in RStudio:

::: {.nonincremental}

- Go to **Profile -> Start profiling**
- Now interactively run the code you want to profile
- Go to **Profile -> Stop profiling** to see the results

:::

![](img/profiling_rstudio.png)

## Benchmarking R code

Which version of the code is faster?

. . .


::: {.columns}

::: {.column}

```{r}
#| label: define-center_data_slow
center_data_slow <- function() {
  # Create data with 150 columns and 100000 rows
  df <- as.data.frame(matrix(rnorm(150 * 100000), nrow = 100000))

  # Calculate mean of each column
  means <- apply(df, 2, mean)

  # Subtract means
  for (i in seq_along(means)) {
    df[, i] <- df[, i] - means[i]
    # Simulate inefficiency by sleeping 0.01 seconds
    Sys.sleep(0.01)
  }
  return(df)
}
```

:::


::: {.column}

```{r}
#| label: define-center_data_fast
center_data_fast <- function() {
  # Create data with 150 columns and 100000 rows
  df <- matrix(rnorm(150 * 100000), nrow = 100000)

  # Calculate mean of each column
  means <- colMeans(df)

  # Subtract means
  for (i in seq_along(means)) {
    df[, i] <- df[, i] - means[i]
  }
  return(df)
}
```

:::

:::

## Benchmarking R code - the easy way

Use the `system.time()` function for simple function calls:

```{r}
system.time(center_data_slow())
system.time(center_data_fast())
```

. . .

Use the `tictoc` package also for longer code sections:

```{r}
#| label: tic-toc
# install.packages("tictoc")
library(tictoc)

tic()
slow_data <- center_data_slow()
toc()

tic()
fast_data <- center_data_fast()
toc()
```

## Benchmarking R code

Use the `microbenchmark` package to compare the functions:

. . .

```{r}
#| label: benchmark-center-data
#| code-line-numbers: 1-8|9-15|9-21
# install.packages("microbenchmark")
library(microbenchmark)

runtime_comp <- microbenchmark(
  slow = center_data_slow(),
  fast = center_data_fast(),
  times = 10 # the default is 100 but we are impatient
)

runtime_comp

# look at relative runtime comparison
summary(runtime_comp, unit = "relative")
```

## Benchmarking R code

We can look at benchmarking results using ggplot

```{r}
#| label: autoplot-compare-functions
#| output-location: fragment
#| out-width: "80%"
library(ggplot2)
autoplot(runtime_comp)
```

# Optimize your R code{.inverse}

:::{.nonincremental}

- Basic principles
- Data analysis bottlenecks
- Advanced optimization: Parallelization and C++
  
:::

# Basic principles

## Vectorize your code

- Vectors are central to R programming
- R is optimized for vectorized code
  - Implemented directly in C/Fortran
- Vector operations can often replace for-loops in R
- If there is a vectorized version of a function: Use it

## Vectorized functions in R

Some examples of base-R vectorized functions:

:::{.nonincremental}

- **Arithmetic & Math**: `+`, `-`, `*`, `/`, `log()`, `exp()`, `sqrt()`, `mean()`, `sd()`, `rowSums()`, `colMeans()`
- **Logical & Comparison**: `<`, `>`, `==`, `!=`, `&`, `|`, `all()`, `any()`, `%in%`
- **Data Manipulation**: `[`, `ifelse()`, `cut()`, `cumsum()`, `which()`, `match()`, `order()`
- **String operations**: `paste()`, `paste0()`, `grep()`, `gsub()` 
:::

## Vectorize your code

::: {.panel-tabset}

## Vector arithmetic vs. for-loops

```{r}
#| label: benchmark-vectorization
#| output-location: fragment
x <- 1:1e6
y <- 1:1e6

microbenchmark(
  for_loop = {
    result <- numeric()
    for (i in seq_along(x)) {
      result[i] <- x[i] * 2 + y[i] / 2
    }
  },
  vectorized = x * 2 + y / 2,
  times = 10
)
```

## Example 2: Calculating cumulative values

```{r}
#| label: benchmark-cumsum
#| output-location: fragment
x <- sample(1:100, 1e5, replace = TRUE)

microbenchmark(
  for_loop = {
    result <- numeric()
    sum_so_far <- 0
    for (i in seq_along(x)) {
      sum_so_far <- sum_so_far + x[i]
      result[i] <- sum_so_far
    }
  },
  vectorized = cumsum(x),
  times = 10
)
```

:::

## For-loops in R

- For-loops are **relatively slow** and it is easy to make them even slower with bad design
- Often they are used when vectorized code would be better

- For loops can often be replaced, e.g. by
  - Functions from the apply family (e.g. `apply`, `lapply`, ...)
  - Vectorized functions (e.g. `sum`, `colMeans`, ...)
  - Vectorized functions from the [`purrr` package](https://purrr.tidyverse.org/) (e.g. `map`)
  
. . .

But: For loops are not necessarily bad, **sometimes** they are the **best solution** and **more readable** than vectorized code.

## Don't grow objects in a loop

If you know how big your object will be, pre-allocate it.

::: {.columns}
::: {.column}

```{r}
#| label: create-f1-loop
f1 <- function() {
  x <- numeric() # no pre-allocation
  for (i in 1:1e6) {
    x[i] <- i
  }
}
```

:::
::: {.column}

```{r}
#| label: create-f2-loop
f2 <- function() {
  x <- numeric(1e6) # pre-allocate vector
  for (i in 1:1e6) {
    x[i] <- i
  }
}

```

:::
::: 

. . .

<br>

```{r}
#| label: benchmark-f1-f2-loop
#| output-location: fragment

compare_alloc <- microbenchmark(
  no_alloc = f1(),
  pre_alloc = f2(),
  times = 10
)

summary(compare_alloc, unit = "relative")
```

## Don't grow objects in a loop

### Why?

R makes a copy of the object each time it grows, which is inefficient.

## Don't grow objects in a loop

```{r}
#| label: trace-memory
x <- numeric()
y <- numeric(10)

for (i in 1:10) {
  x[i] <- i
  y[i] <- i

  # Print current memory location after each assignment
  cat(
    "After iteration ",
    i,
    ": x is in ",
    pryr::address(x),
    "and y is in ",
    pryr::address(y),
    "\n"
  )
}
```

## Cache variables

If you use a value multiple times, store it in a variable to avoid re-calculation

. . .

**Example**: Calculate column means and normalize them by the standard deviation

. . .

```{r}
#| label: cache-variables
#| code-line-numbers: 1-3|4-10|11-14
# A matrix with 1000 columns
x <- matrix(rnorm(10000), ncol = 1000)

microbenchmark(
  no_cache = apply(x, 2, function(i) mean(i) / sd(x)),
  cache = {
    sd_x <- sd(x)
    apply(x, 2, function(i) mean(i) / sd_x)
  }
)
```

# Efficient data analysis

## Efficient workflow

- Filter early: Remove unnecessary rows/columns at the beginning
- Avoid redundant calculations
  - Calculate values once and store them in variables
  - Save intermediate results
- Use efficient data formats
  - Text files instead of Excel files
  - Binary formats like Parquet and feather for large datasets
  - Consider databases for very large datasets
- Use the right packages and functions for your tasks

## tidyverse

::: {.columns}
::: {.column width=70%}
- Collection of packages for data analysis
- Consistent syntax, works well with pipes (`|>`, `%>%`)
- Main data structure `tbl_df` 
- Universe of efficient packages build around the core tidyverse, e.g.
  - `dbplyr` for database and `dtplyr` for data.table backends
  - `futureverse` packages for parallelization
- Often fast enough but can be slower than alternatives

:::
::: {.column width=30%}
![](img/stickr/tidyverse.png)
:::
::: 


## data.table

::: {.columns}
::: {.column width=70%}
- Functions for I/O, data manipulation and more
- Popular alternative to the `tidyverse`
- Designed for fast and memory-efficient data analysis
- Syntax quite different from tidyverse
:::
::: {.column width=30%}

![](img/stickr/datatable.png)
:::
:::


## collapse

::: {.columns}
::: {.column width=70%}
> collapse is written in C and C++, with algorithms much faster than base R‚Äôs, has extremely low evaluation overheads, scales well, and excels on complex statistical tasks.

- Data manipulation and statistical computing
- Easy to integrate with existing R code
  - works well with both `data.table` and `dplyr` workflows/data structures
  - often prefixes functions with `f` like `fmean`, `fmutate`, `fsummarize`, ...
:::
::: {.column width=30%}
![](img/stickr/collapse.png)
:::
:::


## Arrow

::: {.columns}
::: {.column width=70%}
- Access to features of Apache Arrow C++ library
- Functions for I/O and data manipulation
- Access to arrow data formats like parquet, feather, etc.
- Can also operate on larger-than-RAM data
- Provide Arrow C++ backend to `dplyr`
:::
::: {.column width=30%}
![](img/stickr/arrow.png)

:::
:::

## Other packages :construction:

Of course there are so many more packages like:

:::{.nonincremental}

- [`polars`](https://pola-rs.github.io/r-polars/): Fast DataFrame library implemented in Rust
- [`fst`](https://www.fstpackage.org/): Fast and efficient data format with high read/write speed
- ...

:::

Do you know others? Let's collect them in the chat

. . .

Check the resources section of the README for all links.

## Read data - text and excel

**Example:** Read data on global greenhouse gas emissions (~320000 rows, 8 cols).

```{r}
#| label: warmup-reading
#| echo: false
file_path_csv <- here::here("data/ghg_ems.csv")
file_path_xlsx <- here::here("data/ghg_ems.xlsx")
file_path_parquet <- here::here("data/ghg_ems.parquet")
file_path_fst <- here::here("data/ghg_ems.fst")

dummy <- readr::read_csv(file_path_csv)
dummy <- read.csv(file_path_csv)
dummy <- data.table::fread(file_path_csv)
dummy <- arrow::read_csv_arrow(file_path_csv)
dummy <- arrow::read_parquet(file_path_parquet)
dummy <- fst::read_fst(file_path_fst)
rm(dummy)
```

. . .

```{r}
#| label: benchmark-reading-data
#| message: false
#| output-location: slide
file_path_csv <- here::here("data/ghg_ems.csv")
file_path_xlsx <- here::here("data/ghg_ems.xlsx")

compare_read <- microbenchmark(
  # base R
  read.csv = read.csv(file_path_csv),

  # tidyverse
  read_csv = readr::read_csv(file_path_csv, show_col_types = FALSE),
  read_excel = readxl::read_excel(file_path_xlsx),

  # data.table
  fread = data.table::fread(file_path_csv, showProgress = FALSE),

  # arrow
  read_csv_arrow = arrow::read_csv_arrow(file_path_csv),
  times = 10
)

autoplot(compare_read)
```

## Read data - binary

Binary file types can be faster (especially as file size increases)

. . .

```{r}
#| label: benchmark-reading-binary
#| output-location: slide
file_path_parquet <- here::here("data/ghg_ems.parquet")
file_path_fst <- here::here("data/ghg_ems.fst")

compare_read_binary <- microbenchmark(
  # tidyverse
  read_csv = readr::read_csv(file_path_csv, show_col_types = FALSE),

  # data.table
  read_fread = data.table::fread(file_path_csv, showProgress = FALSE),

  # arrow parquet format
  read_parquet = arrow::read_parquet(file_path_parquet),

  # fst format (efficient R only format)
  read_fst = fst::read_fst(file_path_fst),

  times = 10
)

autoplot(compare_read_binary)
```

## Write data

Every corresponding read function has a write counterpart:

:::{.fragement}

::: {.nonincremental}

- `write_csv` from the `readr` package (tidyverse)
- `fwrite` from the `data.table` package
- `write_csv_arrow`, `write_parquet` from the `arrow` package
- `write_fst` from the `fst` package

:::

:::

## Write data

```{r}
#| label: benchmark-writing-data
#| output-location: slide

# create an example with 1000000 rows to export
df <- data.frame(x = 1:1000000, y = 1:1000000, z = 1:1000000)

compare_output <- microbenchmark::microbenchmark(
  # write Excel
  write_excel = writexl::write_xlsx(df, "df.xlsx"),

  # write text
  write.csv = write.csv(df, "df.csv"),
  write_csv = readr::write_csv(df, "df.csv"),
  fwrite = data.table::fwrite(df, "df.csv"),
  write_csv_arrow = arrow::write_csv_arrow(df, "df.csv"),

  # write binary formats
  write_parquet = arrow::write_parquet(df, "data/df.parquet"),
  write_fst = fst::write_fst(df, "data/df.fst"),
  times = 10
)

autoplot(compare_output)
```

## Data manipulation

**Example**: Summarize mean carbon emissions from Electricity by Country

<br>

```{r}
#| label: setup-summarize-by-group
#| message: false
library(data.table)
library(dplyr)
library(collapse)
library(arrow)

# Use the right data formats
ghg_ems <- readr::read_csv(file_path_csv)
ghg_ems_dt <- setDT(ghg_ems) # to data table
ghg_ems_parquet <- read_parquet(file_path_parquet, as_data_frame = FALSE)
```

## Summarize data by group

```{r}
#| label: functions-summarize-by-group
#| code-line-numbers: 1-4|6-11|13-18|20-26
# 1. The data table way
summarize_dt <- function() {
  ghg_ems_dt[, mean(Electricity, na.rm = TRUE), by = Country]
}

# 2. The dplyr way
summarize_dplyr <- function() {
  ghg_ems |>
    group_by(Country) |>
    summarize(mean_e = mean(Electricity, na.rm = TRUE))
}

# 3. The collapse way
summarize_collapse <- function() {
  ghg_ems |>
    fgroup_by(Country) |>
    fsummarise(mean_e = fmean(Electricity))
}

# 4. The arrow way
summarize_arrow <- function() {
  ghg_ems_parquet |>
    group_by(Country) |>
    summarize(mean_e = mean(Electricity, na.rm = TRUE)) |>
    collect() # result is only collected on request
}
```

## Efficient data manipulation

**Example**: Summarize mean carbon emissions from Electricity by Country

```{r}
#| label: benchmark-summarizing-by-group
# compare the speed of all versions
compare_summarize <- microbenchmark(
  dplyr = summarize_dplyr(),
  data_table = summarize_dt(),
  collapse = summarize_collapse(),
  arrow = summarize_arrow(),
  times = 10
)

summary(compare_summarize, unit = "relative")
```

# Advanced optimization {.inverse}

> Parallelization and C++

## Parallelization

By default, R works on one core but CPUs have multiple cores

. . .

```{r}
#| label: find-available-cores
# Find out how many cores you have with the parallel package
# install.packages("parallel")
parallel::detectCores()
```

. . .

::::{.columns}

::: {.column width="50%"}

Sequential

![](img/sequential.png){width="40%" fig-align="center"}

:::

::: {.column width="50%"}

:::{.fragment}

Parallel

![](img/parallel.png)

:::

:::

::::

## Parallelization with the futureverse

- `future` is a framework to help you parallelize existing R code
  - Parallel versions of base R apply family
  - Parallel versions of `purrr` functions
  - Parallel versions of `foreach` loops


## A slow example

Let's create a very slow square root function

```{r}
#| label: slow-sqrt
slow_sqrt <- function(x) {
  Sys.sleep(1) # simulate 1 second of computation time
  sqrt(x)
}
```

. . .

Before you run anything in parallel, tell R how many cores to use:

```{r}
#| label: plan-parallel
# Load future package
library(future)
# Plan parallel session with 5 cores
plan(multisession, workers = 5)
```	

## Parallel apply functions

To run the function on a vector of numbers we could use

:::{.columns}

:::{.column width="50%"}

**Sequential** **`lapply`** <br>

```{r}
#| label: lapply-slow-sqrt
#| message: false
# create a vector of 10 numbers
x <- 1:10
tic()
result <- lapply(x, slow_sqrt)
toc()
```

:::

:::{.column width="50%"}

:::{.fragment}

**Parallel** **`future_lapply`** <br>

```{r}
#| label: future-lapply-slow-sqrt
#| eval: false
# Load future.apply package
library(future.apply)

tic()
result <- future_lapply(x, slow_sqrt)
toc()
#> 2.6 sec elapsed
```

:::

:::

:::

## Parallel apply functions

Selected base R apply functions and their future versions:

```{r}
#| label: apply-functions-table
#| echo: false
# A data. frame with two columns: base and future.apply
# In the base column there are the base R functions and in the future.apply
# column there are the future versions of the functions
# The function names should be enclosed in backticks and then in quotation marks to be printed nicely
apply_functions <- data.frame(
  base = c(
    "`
lapply
`",
    "`
sapply
`",
    "`
vapply
`",
    "`
mapply
`",
    "`
tapply
`",
    "`
apply
`",
    "`
Map
`"
  ),
  future.apply = c(
    "`
future_lapply
`",
    "`
future_sapply
`",
    "`
future_vapply
`",
    "`
future_mapply
`",
    "`
future_tapply
`",
    "`
future_apply
`",
    "`
future_Map
`"
  )
)
# print the data frame apply_functions nicely in the quarto doc using knitr
# Apply striped design to the table
knitr::kable(apply_functions)
```

## Parallel for loops

A normal for loop:

```{r}
#| label: for-loop-slow-sqrt
#| eval: false
z <- list()
for (i in 1:10) {
  z[i] <- slow_sqrt(i)
}
```

. . .

First, change into a `foreach`

```{r}
#| label: foreach-slow-sqrt
#| eval: false
library(foreach)
z <- foreach(i = 1:10) %do%
  {
    slow_sqrt(i)
  }
```

## Parallel for loops

Use `doFuture` and `foreach` package to parallelize for loops

:::{.columns}

:::{.column width="50%"}

The **sequential** version

```{r}
#| label: foreach-slow-sqrt-sequential
library(foreach)

tic()
z <- foreach(i = 1:10) %do%
  {
    slow_sqrt(i)
  }
toc()
```

:::

:::{.column width="50%"}

The **parallel** version

:::{.fragment}

```{r}
#| label: foreach-slow-sqrt-parallel
library(doFuture)

tic()
z <- foreach(i = 1:10) %dofuture%
  {
    slow_sqrt(i)
  }
toc()
```

:::

:::

:::

## Future `purrr` functions

The [`furrr` package](https://furrr.futureverse.org/) offers parallel versions of [`purrr` functions](https://purrr.tidyverse.org/)

. . .

:::{.columns}

:::{.column width="50%"}

The **sequential** version

```{r}
#| label: purrr-slow-sqrt
library(purrr)

# the purrr version
tic()
z <- map(x, slow_sqrt)
toc()
```

:::

:::{.column width="50%"}

The **parallel** version

```{r}
#| label: furrr-slow-sqrt
library(furrr)

# the furrr version
tic()
z <- future_map(x, slow_sqrt)
toc()
```

:::

:::

## Close multisession

When you are done working in parallel, explicitly close your multisession:

```{r}
#| label: close-multisession
# close the multisession plan
plan(sequential)
```

## Replace slow code with C++

- Use the [`Rcpp` package](https://www.rcpp.org/) to re-write R functions in C++
 - `Rcpp` is also used internally by many R packages to make them faster
- Requirements: 
  - Install C++ compiler (Rtools for Windows, Xcode for Mac, gcc for Linux, see [here](https://teuder.github.io/rcpp4everyone_en/020_install.html#install-c-compiler) for instructions)
  - Some knowledge of C++
  
- See [this book chapter](https://csgillespie.github.io/efficientR/performance.html#rcpp) and the [online documentation](https://www.rcpp.org/) for more info
 
## Rewrite a function in C++

**Example:** R function to calculate Fibonacci numbers

```{r}
#| label: fibonacci-r
# A function to calculate Fibonacci numbers
fibonacci_r <- function(n) {
  if (n < 2) {
    return(n)
  } else {
    return(fibonacci_r(n - 1) + fibonacci_r(n - 2))
  }
}
```

<br>

. . .

```{r}
#| label: fibonacci-r-30
# Calculate the 30th Fibonacci number
fibonacci_r(30)
```

## Rewrite a function in C++

Use `cppFunction` to rewrite the function in C++:

```{r}
#| label: fibonacci-cpp
library(Rcpp)

# Rewrite the fibonacci_r function in C++
fibonacci_cpp <- cppFunction(
  "int fibonacci_cpp(int n){
    if (n < 2){
      return(n);
    } else {
      return(fibonacci_cpp(n - 1) + fibonacci_cpp(n - 2));
    }
  }"
)
```

<br>

. . .

```{r}
#| label: fibonacci-cpp-30
# calculate the 30th Fibonacci number
fibonacci_cpp(30)
```

## Rewrite a function in C++

You can also source C++ functions from C++ scripts.

. . .

C++ script `fibonacci.cpp`:

```{cpp eval = FALSE}
#include "Rcpp.h"

// [[Rcpp::export]]
int fibonacci_cpp(const int x) {
   if (x < 2) return(x);
   return (fibonacci(x - 1)) + fibonacci(x - 2);
}
```

. . .

Then source the function in your R script using `sourceCpp`:

. . .

```{r eval=FALSE}
sourceCpp("fibonacci.cpp")

# Use the function in your R script like you are used to
fibonacci_cpp(30)
```


## How much faster is C++?

```{r}
#| label: benchmark-fibonacci
compare_rcpp <- microbenchmark(
  r = fibonacci_r(30),
  rcpp = fibonacci_cpp(30),
  times = 10
)

summary(compare_rcpp, unit = "relative")
```

# Summary {.inverse}

## Efficient R code and optimization :construction:

- Find bottlenecks and optimize those
- Many quick wins, e.g.
  - Use `fread` instead of `read.csv`
  - Use `future_apply/future_map` instead of `apply/map`
  - Replace some tidyverse functions with collapse functions

## Summary

![](img/pyramid_text.png){width=90%}


## Thank you for your attention :)
Questions?

# 6Appendix

## Cache function results

- Use the [`memoise` package](https://memoise.r-lib.org/)
- If functions are called many times with the same arguments
  - Avoids the recalculation
- Useful to e.g. improve the performance of a shiny app

## Cache function results

**Example**: Create a plot on a subset of the `iris` data set

```{r}
#| label: setup-memoise
# Example of using memoise to cache results
library(memoise)
library(ggplot2)

# Remove rows from plotting function
select_iris_species <- function(rows_to_remove) {
  iris_subset <- iris[-rows_to_remove, ]
  # Do a plot on the subset
  p <- ggplot(
    iris_subset,
    aes(x = Sepal.Length, y = Sepal.Width, color = Species)
  ) +
    geom_point()
}

# Version of the function with memoise
select_iris_species_mem <- memoise(select_iris_species)
```

## Cache function results

**Example**: Create a plot on a subset of the `iris` data set

```{r}
#| label: benchmark-iris-species
#| output-location: column
# Compare the two versions
result <- microbenchmark(
  no_cache = select_iris_species(10),
  cache = select_iris_species_mem(10)
)

autoplot(result)
```
